import sys
import time
import pytz
from datetime import datetime
import logging

from utils import (
    get_daily_papers_by_keyword_with_retries,
    generate_table,
    back_up_files,
    restore_files,
    remove_backups,
    get_daily_date,
)


# è®¾ç½®åŸºæœ¬æ—¥å¿—
logging.basicConfig(level=logging.INFO)

# ä½¿ç”¨æ—¥å¿—è€Œä¸æ˜¯æ‰“å°
logging.info("è„šæœ¬å¼€å§‹æ‰§è¡Œ")
# åœ¨æ¯ä¸ªå…³é”®æ­¥éª¤æ·»åŠ æ—¥å¿—

beijing_timezone = pytz.timezone("Asia/Shanghai")

# NOTE: arXiv API seems to sometimes return an unexpected empty list.

# get current beijing time date in the format of "2021-08-01"
current_date = datetime.now(beijing_timezone).strftime("%Y-%m-%d")
# get last update date from README.md
with open("README.md", "r") as f:
    while True:
        line = f.readline()
        if "Last update:" in line:
            break
    last_update_date = line.split(": ")[1].strip()
    # if last_update_date == current_date:
    # sys.exit("Already updated today!")

logging.info("è·å–å…³é”®è¯åˆ—è¡¨")

keywords = [
    "Vision and Language Navigation",
    "Vision Language Action",
    "World Model",
    "Visual SLAM",
    "Visual Inertial SLAM",
    "Visual Inertial Odometry",
    "Lidar SLAM",
    "LiDAR Odometry",
    "GNSS",
    "Graph Optimization",
    "Dynamic SLAM",
    "Semantic SLAM",
    "Gaussian SLAM",
    "Autonomous Driving",
    "Kalman Filter",
    "Loop Closure Detection",
    "Visual Place Recognition",
    "3D Gaussian Splatting",
    "Deep Learning",
    "LLM",
]  # TODO add more keywords

max_result = 80  # maximum query results from arXiv API for each keyword
issues_result = 10  # maximum papers to be included in the issue

# all columns: Title, Authors, Abstract, Link, Tags, Comment, Date
# fixed_columns = ["Title", "Link", "Date"]

column_names = ["Title", "Link", "Abstract", "Date", "Comment"]

back_up_files()  # back up README.md and ISSUE_TEMPLATE.md

logging.info("è·å–æ¯æ—¥è®ºæ–‡")
# write to README.md
f_rm = open("README.md", "w", encoding="utf-8")
f_rm.write(
    """<div align="center">

# ğŸš€ Embodied-AI-Daily

_Automatically fetches the latest arXiv papers on **VLN Â· VLA Â· SLAM Â· 3D Â· Embodied AI**_

<p>
  <img src="https://img.shields.io/badge/Update-Daily-brightgreen.svg" alt="æ¯æ—¥æ›´æ–°">
  <img src="https://img.shields.io/badge/Source-arXiv-red.svg" alt="æ¥æºï¼šarXiv">
  <img src="https://img.shields.io/badge/Papers-VLNÂ·VLAÂ·SLAMÂ·3D-blue.svg" alt="è®ºæ–‡ä¸»é¢˜ï¼šVLNÂ·VLAÂ·SLAMÂ·3D">
  <img src="https://img.shields.io/github/stars/luohongk/Embodied-AI-Daily?style=social" alt="GitHub Stars">
  <a href="https://github.com/luohongk" target="_blank">
    <img src="https://img.shields.io/badge/Author-luohongkun-blueviolet.svg" alt="ä½œè€…ï¼šluohongk">
  </a
  <a href="https://luohongkun.top/me/" target="_blank">
    <img src="https://img.shields.io/badge/Homepage-www.luohongkun.top/me/-9cf.svg" alt="ä¸»é¡µï¼šGitHub">
  </a>
</p>


</div>

---

## ğŸ“Œ About
This project automatically fetches the latest papers from **arXiv** based on predefined keywords.  
- Each section in the README corresponds to a **search keyword**.  
- Only the most recent papers are kept (up to **100 per keyword**).  
- Click **Watch** (ğŸ‘€) on the repo to get **daily email notifications**.

_Last update: {0}_

---
""".format(current_date)
)
logging.info("ç”Ÿæˆreadme")
# write to ISSUE_TEMPLATE.md
f_is = open(".github/ISSUE_TEMPLATE.md", "w")  # file for ISSUE_TEMPLATE.md
f_is.write("---\n")
f_is.write("title: Latest {0} Papers - {1}\n".format(issues_result, get_daily_date()))
f_is.write("labels: documentation\n")
f_is.write("---\n")
f_is.write(
    "**Please check the [Github](https://github.com/luohongk/DailyArXiv) page for a better reading experience and more papers.**\n\n"
)

for keyword in keywords:
    logging.info(f"æ­£åœ¨å¤„ç†å…³é”®è¯: {keyword}")
    f_rm.write("## {0}\n".format(keyword))
    f_is.write("## {0}\n".format(keyword))
    if len(keyword.split()) == 1:
        link = "AND"  # for keyword with only one word, We search for papers containing this keyword in both the title and abstract.
    else:
        link = "OR"
    papers = get_daily_papers_by_keyword_with_retries(
        keyword, column_names, max_result, link
    )
    logging.info("æˆåŠŸè·å–è®ºæ–‡")
    if papers is None:  # failed to get papers
        logging.error(f"æœªèƒ½è·å–å…³é”®è¯ '{keyword}' çš„è®ºæ–‡ï¼")
        print("Failed to get papers!")
        f_rm.close()
        f_is.close()
        restore_files()
        sys.exit("Failed to get papers!")

    logging.info(f"æˆåŠŸè·å– {len(papers)} ç¯‡è®ºæ–‡ï¼Œæ­£åœ¨ç”Ÿæˆè¡¨æ ¼ã€‚")
    rm_table = generate_table(papers)
    is_table = generate_table(papers[:issues_result], ignore_keys=["Abstract"])
    f_rm.write(rm_table)
    f_rm.write("\n\n")
    f_is.write(is_table)
    f_is.write("\n\n")
    time.sleep(7)  # avoid being blocked by arXiv API

f_rm.close()
f_is.close()
remove_backups()
